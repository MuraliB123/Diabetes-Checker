{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7Ga4r0XNqUmEQWMiYwASp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuraliB123/Diabetes-Checker/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spzxgilZRwCb",
        "outputId": "becb60b0-c04e-4ba1-f2d9-8c770ba325dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age std: [0.         0.33333333 0.18181818 0.87878788 0.09090909 0.51515152\n",
            " 0.96969697 1.         0.84848485]\n",
            "Income std [0.88024813 0.01191226 0.19369897 0.         0.92469322 0.39953312\n",
            " 0.19090055 1.         0.90275882]\n",
            "First 9 neighbor distances:\n",
            "(36119.39848662488, 7)\n",
            "(36119.47544070703, 4)\n",
            "(36119.49600063277, 8)\n",
            "(36119.520051728214, 0)\n",
            "(36119.99982829707, 5)\n",
            "(36120.206267975, 2)\n",
            "(36120.207639313034, 6)\n",
            "(36120.38777860667, 1)\n",
            "(36120.398703334766, 3)\n",
            "Predicted class: Bad loss\n",
            "Age std: [-1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
            "Income std [1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "First 9 neighbor distances:\n",
            "(36186.34, 0)\n",
            "(36186.34, 1)\n",
            "(36186.34, 2)\n",
            "(36186.34, 3)\n",
            "(36186.34, 4)\n",
            "(36186.34, 5)\n",
            "(36186.34, 6)\n",
            "(36186.34, 7)\n",
            "(36186.34, 8)\n",
            "Predicted class: Bad loss\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "data = pd.read_csv('/content/Data.csv')\n",
        "data = data.drop([\"Record\"], axis=1)\n",
        "data = data.drop([\"Marital\"],axis=1)\n",
        "Y = data[\"Risk\"]\n",
        "data = data.drop([\"Risk\"],axis=1)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data)\n",
        "data_std1 = scaler.transform(data)\n",
        "data = np.array(data)\n",
        "data_1=stats.zscore(data, axis=1)\n",
        "\n",
        "print(\"Age std:\", data_std1[:, 0])\n",
        "print(\"Income std\",data_std1[:,1])\n",
        "\n",
        "test_10 = [66,36120.34]\n",
        "k = 9\n",
        "distances = []\n",
        "for i in range(len(data_std1)):\n",
        "    distance = np.sqrt(np.sum((test_10 - data_std1[i])**2))\n",
        "    distances.append((distance, i))\n",
        "distances.sort()\n",
        "print(\"First 9 neighbor distances:\")\n",
        "for i in range(9):\n",
        "    print(distances[i])\n",
        "nearest_neighbors = [d[1] for d in distances[:9]]\n",
        "neighbor_classes = Y[nearest_neighbors]\n",
        "predicted_class = neighbor_classes.mode()[0]\n",
        "print(\"Predicted class:\", predicted_class)\n",
        "###################z_score##########################\n",
        "print(\"Age std:\", data_1[:, 0])\n",
        "print(\"Income std\",data_1[:,1])\n",
        "\n",
        "test_10 = [66,36120.34]\n",
        "k = 9\n",
        "distances = []\n",
        "for i in range(len(data_1)):\n",
        "    distance = np.sum(np.abs(test_10 - data_1[i]))\n",
        "    distances.append((distance, i))\n",
        "distances.sort()\n",
        "print(\"First 9 neighbor distances:\")\n",
        "for i in range(9):\n",
        "    print(distances[i])\n",
        "nearest_neighbors = [d[1] for d in distances[:9]]\n",
        "neighbor_classes = Y[nearest_neighbors]\n",
        "predicted_class = neighbor_classes.mode()[0]\n",
        "print(\"Predicted class:\", predicted_class)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler #for sub (iv)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Min-Max Standardization\n",
        "def min_max_scaling(dataset, feature):\n",
        "    min_val = dataset[feature].min()\n",
        "    max_val = dataset[feature].max()\n",
        "    return (dataset[feature] - min_val) / (max_val - min_val)\n",
        "\n",
        "\n",
        "\n",
        "#caculating distances\n",
        "def euclid(dataset):\n",
        "  new_record=dataset.iloc[-1,:]\n",
        "  dataset=dataset.iloc[:-1,:]#removing last row\n",
        "  dataset['Distance'] = np.sqrt((dataset['Sepal-length'] - new_record['Sepal-length'])**2 + (dataset['Sepal-width'] - new_record['Sepal-width'])**2 +\n",
        "                                (dataset['Petal-length'] - new_record['Petal-length'])**2 + (dataset['Petal-width'] - new_record['Petal-width'])**2)\n",
        "\n",
        "  # Sort the data by distance and select the top k=9 neighbors\n",
        "  k = 5\n",
        "  nearest_neighbors = dataset.sort_values(by='Distance')[:k]\n",
        "\n",
        "  # Count the class occurrences among the nearest neighbors\n",
        "  class_counts = nearest_neighbors['Ground-truth'].value_counts()\n",
        "\n",
        "  # Classify the new record using unweighted voting\n",
        "  predicted = class_counts.idxmax()\n",
        "\n",
        "  print(f\"The predicted class for the new record is: {predicted}\")\n",
        "  print(dataset['Distance'])\n",
        "\n",
        "def manhat(dataset):\n",
        "  new_record=dataset.iloc[-1,:]\n",
        "  dataset=dataset.iloc[:-1,:]#removing last row\n",
        "  dataset['Distance'] = (abs(dataset['Sepal-length'] - new_record['Sepal-length']) + abs(dataset['Sepal-width'] - new_record['Sepal-width']) +\n",
        "                                abs(dataset['Petal-length'] - new_record['Petal-length']) + abs(dataset['Petal-width'] - new_record['Petal-width']))\n",
        "\n",
        "  # Sort the data by distance and select the top k=9 neighbors\n",
        "  k = 5\n",
        "  nearest_neighbors = dataset.sort_values(by='Distance')[:k]\n",
        "\n",
        "  # Count the class occurrences among the nearest neighbors\n",
        "  class_counts = nearest_neighbors['Ground-truth'].value_counts()\n",
        "\n",
        "  # Classify the new record using unweighted voting\n",
        "  predicted = class_counts.idxmax()\n",
        "\n",
        "  print(f\"The predicted class for the new record is: {predicted}\")\n",
        "  print(dataset['Distance'])\n",
        "\n",
        "\n",
        "dataset = pd.read_csv(\"/content/data4_19(1).csv\")\n",
        "\n",
        "dataset['Sepal-length'] = min_max_scaling(dataset, 'Sepal-length')\n",
        "dataset['Sepal-width'] = min_max_scaling(dataset, 'Sepal-width')\n",
        "dataset['Petal-length'] = min_max_scaling(dataset, 'Petal-length')\n",
        "dataset['Petal-width'] = min_max_scaling(dataset, 'Petal-width')\n",
        "\n",
        "euclid(dataset)\n",
        "print('\\n\\n\\n')\n",
        "manhat(dataset)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {'n_neighbors': [1, 3, 5, 7, 10, 11, 13, 15]}\n",
        "\n",
        "# Create the KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv(\"/content/data4_19(1).csv\")\n",
        "\n",
        "# Get the target column\n",
        "target = dataset['Ground-truth']\n",
        "\n",
        "# Create the OneHotEncoder object\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "target = np.array(target)\n",
        "# Fit the encoder on the target column\n",
        "encoder.fit(target.reshape(-1,1))\n",
        "\n",
        "# Transform the target column into one-hot encoded features\n",
        "encoded_target = encoder.transform(target)\n",
        "\n",
        "# Add the encoded target features to the dataset\n",
        "dataset['Ground-truth'] = encoded_target.toarray()\n",
        "\n",
        "# Fit the GridSearchCV object on the training data\n",
        "grid_search.fit(dataset, dataset['Ground-truth'])\n",
        "\n",
        "# Get the best k value\n",
        "best_k = grid_search.best_params_['n_neighbors']\n",
        "\n",
        "# Print the best k value\n",
        "print(\"The best k value is:\", best_k)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mL61Uw54jLkF",
        "outputId": "1a620e13-74ae-4b76-ecec-0f3f44127e8b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted class for the new record is: Iris-setosa\n",
            "0      0.168966\n",
            "1      0.093169\n",
            "2      0.145991\n",
            "3      0.167526\n",
            "4      0.215614\n",
            "         ...   \n",
            "145    1.164386\n",
            "146    1.014843\n",
            "147    1.053309\n",
            "148    1.148063\n",
            "149    0.936625\n",
            "Name: Distance, Length: 150, dtype: float64\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The predicted class for the new record is: Iris-setosa\n",
            "0      0.194444\n",
            "1      0.125000\n",
            "2      0.197505\n",
            "3      0.183616\n",
            "4      0.263889\n",
            "         ...   \n",
            "145    1.977401\n",
            "146    1.874058\n",
            "147    1.796846\n",
            "148    1.955744\n",
            "149    1.529896\n",
            "Name: Distance, Length: 150, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-943cb6bd6836>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset['Distance'] = np.sqrt((dataset['Sepal-length'] - new_record['Sepal-length'])**2 + (dataset['Sepal-width'] - new_record['Sepal-width'])**2 +\n",
            "<ipython-input-34-943cb6bd6836>:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset['Distance'] = (abs(dataset['Sepal-length'] - new_record['Sepal-length']) + abs(dataset['Sepal-width'] - new_record['Sepal-width']) +\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_features\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" with shape {X.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Unable to find the number of features from X of type numpy.ndarray with shape (151,)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-943cb6bd6836>\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# Transform the target column into one-hot encoded features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mencoded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Add the encoded target features to the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;34m\"infrequent_if_exist\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         }\n\u001b[0;32m--> 917\u001b[0;31m         X_int, X_mask = self._transform(\n\u001b[0m\u001b[1;32m    918\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[1;32m    153\u001b[0m     ):\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         X_list, n_samples, n_features = self._check_X(\n\u001b[1;32m    157\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreset\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_features_in_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    370\u001b[0m                     \u001b[0;34m\"X does not contain any features, but \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                     \u001b[0;34mf\"{self.__class__.__name__} is expecting \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X does not contain any features, but OneHotEncoder is expecting 1 features"
          ]
        }
      ]
    }
  ]
}